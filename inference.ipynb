{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Notebook for inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "\n",
    "# import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from attrdict import AttrDict\n",
    "from diffusion.respace import SpacedDiffusion\n",
    "from model.cfg_sampler import ClassifierFreeSampleModel\n",
    "from model.diffusion import FiLMTransformer\n",
    "from utils.misc import fixseed\n",
    "from utils.model_util import create_model_and_diffusion, load_model\n",
    "from visualize.render_codes import BodyRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds model checkpoint in path (should only be one .pt file)\n",
    "def find_pt(path):\n",
    "    return [file for file in os.listdir(path) if file.endswith('.pt')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model000155000.pt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pt('/nfshomes/vla/audio2photoreal/checkpoints/diffusion/c1_face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_DICT = {\n",
    "    'PXB184': 'c1',\n",
    "    'RLW104': 'c2',\n",
    "    'TXB805': 'c3',\n",
    "    'GQS883': 'c4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from demo.GradioModel\n",
    "class A2PModel:\n",
    "    def __init__(self, person_id, output_dir):\n",
    "        \n",
    "        self.person_id = person_id\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.name = ID_DICT[person_id]\n",
    "\n",
    "        # model args\n",
    "        face_args= f\"checkpoints/diffusion/{self.name}_face/args.json\"\n",
    "        print('face_args', face_args)\n",
    "        pose_args= f\"checkpoints/diffusion/{self.name}_pose/args.json\"\n",
    "        print('pose_args: ', pose_args)\n",
    "\n",
    "        # model checkpoints\n",
    "        face_ckpt = f\"checkpoints/diffusion/{self.name}_face\"\n",
    "        face_ckpt = os.path.join(face_ckpt, find_pt(face_ckpt))\n",
    "        print('face_ckpt', face_ckpt)\n",
    "\n",
    "        pose_ckpt = f\"checkpoints/diffusion/{self.name}_pose\"\n",
    "        pose_ckpt = os.path.join(pose_ckpt, find_pt(pose_ckpt))\n",
    "        print('pose_ckpt', pose_ckpt)\n",
    "\n",
    "\n",
    "        # face model setup\n",
    "        self.face_model, self.face_diffusion, self.device = self._setup_model(\n",
    "            face_args, face_ckpt\n",
    "        )\n",
    "\n",
    "        # pose model setup\n",
    "        self.pose_model, self.pose_diffusion, _ = self._setup_model(\n",
    "            pose_args, pose_ckpt\n",
    "        )\n",
    "\n",
    "        # load stats for standardization\n",
    "        data_stats_path = f'dataset/{person_id}/data_stats.pth'\n",
    "        stats = torch.load(data_stats_path)\n",
    "        stats[\"pose_mean\"] = stats[\"pose_mean\"].reshape(-1)\n",
    "        stats[\"pose_std\"] = stats[\"pose_std\"].reshape(-1)\n",
    "        self.stats = stats\n",
    "\n",
    "        # set up renderer\n",
    "        config_base = f\"checkpoints/ca_body/data/{person_id}\"\n",
    "        self.body_renderer = BodyRenderer(\n",
    "            config_base=config_base,\n",
    "            render_rgb=True,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _setup_model(\n",
    "        self,\n",
    "        args_path: str,\n",
    "        model_path: str,\n",
    "    ) -> (Union[FiLMTransformer, ClassifierFreeSampleModel], SpacedDiffusion):\n",
    "        \n",
    "        with open(args_path) as f:\n",
    "            args = json.load(f)\n",
    "\n",
    "        args = AttrDict(args)\n",
    "        args.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"running on...\", args.device)\n",
    "        args.model_path = model_path\n",
    "        # args.output_dir = \"/tmp/gradio/\"\n",
    "        args.output_dir = self.output_dir\n",
    "        args.timestep_respacing = \"ddim100\"\n",
    "        if args.data_format == \"pose\":\n",
    "            args.resume_trans = f\"checkpoints/guide/{self.name}_pose/checkpoints\"\n",
    "            args.resume_trans = os.path.join(args.resume_trans, find_pt(args.resume_trans))\n",
    "\n",
    "            print('guide checkpoint:', args.resume_trans)\n",
    "\n",
    "        ## create model\n",
    "        model, diffusion = create_model_and_diffusion(args, split_type=\"test\")\n",
    "        print(f\"Loading checkpoints from [{args.model_path}]...\")\n",
    "        state_dict = torch.load(args.model_path, map_location=args.device)\n",
    "        load_model(model, state_dict)\n",
    "        model = ClassifierFreeSampleModel(model)\n",
    "        model.eval()\n",
    "        model.to(args.device)\n",
    "        return model, diffusion, args.device\n",
    "    \n",
    "\n",
    "    def _replace_keyframes(\n",
    "        self,\n",
    "        model_kwargs: Dict[str, Dict[str, torch.Tensor]],\n",
    "        B: int,\n",
    "        T: int,\n",
    "        top_p: float = 0.97,\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            tokens = self.pose_model.transformer.generate(\n",
    "                model_kwargs[\"y\"][\"audio\"],\n",
    "                T,\n",
    "                layers=self.pose_model.tokenizer.residual_depth,\n",
    "                n_sequences=B,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        tokens = tokens.reshape((B, -1, self.pose_model.tokenizer.residual_depth))\n",
    "        pred = self.pose_model.tokenizer.decode(tokens).detach()\n",
    "        return pred\n",
    "\n",
    "    def _run_single_diffusion(\n",
    "        self,\n",
    "        model_kwargs: Dict[str, Dict[str, torch.Tensor]],\n",
    "        diffusion: SpacedDiffusion,\n",
    "        model: Union[FiLMTransformer, ClassifierFreeSampleModel],\n",
    "        curr_seq_length: int,\n",
    "        num_repetitions: int = 1,\n",
    "    ) -> (torch.Tensor,):\n",
    "        sample_fn = diffusion.ddim_sample_loop\n",
    "        with torch.no_grad():\n",
    "            sample = sample_fn(\n",
    "                model,\n",
    "                (num_repetitions, model.nfeats, 1, curr_seq_length),\n",
    "                clip_denoised=False,\n",
    "                model_kwargs=model_kwargs,\n",
    "                init_image=None,\n",
    "                progress=True,\n",
    "                dump_steps=None,\n",
    "                noise=None,\n",
    "                const_noise=False,\n",
    "            )\n",
    "        return sample\n",
    "\n",
    "    def generate_sequences(\n",
    "        self,\n",
    "        model_kwargs: Dict[str, Dict[str, torch.Tensor]],\n",
    "        data_format: str,\n",
    "        curr_seq_length: int,\n",
    "        num_repetitions: int = 5,\n",
    "        guidance_param: float = 10.0,\n",
    "        top_p: float = 0.97,\n",
    "        # batch_size: int = 1,\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        if data_format == \"pose\":\n",
    "            model = self.pose_model\n",
    "            diffusion = self.pose_diffusion\n",
    "        else:\n",
    "            model = self.face_model\n",
    "            diffusion = self.face_diffusion\n",
    "\n",
    "        all_motions = []\n",
    "        model_kwargs[\"y\"][\"scale\"] = torch.ones(num_repetitions) * guidance_param\n",
    "        model_kwargs[\"y\"] = {\n",
    "            key: val.to(self.device) if torch.is_tensor(val) else val\n",
    "            for key, val in model_kwargs[\"y\"].items()\n",
    "        }\n",
    "        if data_format == \"pose\":\n",
    "            model_kwargs[\"y\"][\"mask\"] = (\n",
    "                torch.ones((num_repetitions, 1, 1, curr_seq_length))\n",
    "                .to(self.device)\n",
    "                .bool()\n",
    "            )\n",
    "            model_kwargs[\"y\"][\"keyframes\"] = self._replace_keyframes(\n",
    "                model_kwargs,\n",
    "                num_repetitions,\n",
    "                int(curr_seq_length / 30),\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        sample = self._run_single_diffusion(\n",
    "            model_kwargs, diffusion, model, curr_seq_length, num_repetitions\n",
    "        )\n",
    "        all_motions.append(sample.cpu().numpy())\n",
    "        print(f\"created {len(all_motions) * num_repetitions} samples\")\n",
    "        return np.concatenate(all_motions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(a2p_model:A2PModel, audio: np.ndarray, num_repetitions: int, top_p: float):\n",
    "    # if audio is None:\n",
    "    #     raise gr.Error(\"Please record audio to start\")\n",
    "\n",
    "    sr, y = audio\n",
    "    # set to mono and perform resampling\n",
    "    y = torch.Tensor(y)\n",
    "    if y.dim() == 2:\n",
    "        dim = 0 if y.shape[0] == 2 else 1\n",
    "        y = torch.mean(y, dim=dim)\n",
    "    y = torchaudio.functional.resample(torch.Tensor(y), orig_freq=sr, new_freq=48_000)\n",
    "    sr = 48_000\n",
    "\n",
    "    # NOTE: this part crops audio to 4 seconds for the demo\n",
    "    # # make it so that it is 4 seconds long\n",
    "    # if len(y) < (sr * 4):\n",
    "    #     raise gr.Error(\"Please record at least 4 second of audio\")\n",
    "    # if num_repetitions is None or num_repetitions <= 0 or num_repetitions > 10:\n",
    "    #     raise gr.Error(\n",
    "    #         f\"Invalid number of samples: {num_repetitions}. Please specify a number between 1-10\"\n",
    "    #     )\n",
    "    # cutoff = int(len(y) / (sr * 4))\n",
    "    # y = y[: cutoff * sr * 4]\n",
    "    \n",
    "    curr_seq_length = int(len(y) / sr) * 30\n",
    "    # create model_kwargs\n",
    "    model_kwargs = {\"y\": {}}\n",
    "    dual_audio = np.random.normal(0, 0.001, (1, len(y), 2))\n",
    "    dual_audio[:, :, 0] = y / max(y)\n",
    "    dual_audio = (dual_audio - a2p_model.stats[\"audio_mean\"]) / a2p_model.stats[\n",
    "        \"audio_std_flat\"\n",
    "    ]\n",
    "    model_kwargs[\"y\"][\"audio\"] = (\n",
    "        torch.Tensor(dual_audio).float().tile(num_repetitions, 1, 1)\n",
    "    )\n",
    "    face_results = (\n",
    "        a2p_model.generate_sequences(\n",
    "            model_kwargs, \"face\", curr_seq_length, num_repetitions=int(num_repetitions)\n",
    "        )\n",
    "        .squeeze(2)\n",
    "        .transpose(0, 2, 1)\n",
    "    )\n",
    "    face_results = (\n",
    "        face_results * a2p_model.stats[\"code_std\"] + a2p_model.stats[\"code_mean\"]\n",
    "    )\n",
    "    pose_results = (\n",
    "        a2p_model.generate_sequences(\n",
    "            model_kwargs,\n",
    "            \"pose\",\n",
    "            curr_seq_length,\n",
    "            num_repetitions=int(num_repetitions),\n",
    "            guidance_param=2.0,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        .squeeze(2)\n",
    "        .transpose(0, 2, 1)\n",
    "    )\n",
    "    pose_results = (\n",
    "        pose_results * a2p_model.stats[\"pose_std\"] + a2p_model.stats[\"pose_mean\"]\n",
    "    )\n",
    "    dual_audio = (\n",
    "        dual_audio * a2p_model.stats[\"audio_std_flat\"]\n",
    "        + a2p_model.stats[\"audio_mean\"]\n",
    "    )\n",
    "    return face_results, pose_results, dual_audio[0].transpose(1, 0).astype(np.float32)\n",
    "\n",
    "\n",
    "def audio_to_avatar(a2p_model:A2PModel, audio: np.ndarray, num_repetitions: int, top_p: float):\n",
    "    face_results, pose_results, audio = generate_results(audio, num_repetitions, top_p)\n",
    "    # returns: num_rep x T x 104\n",
    "    B = len(face_results)\n",
    "    results = []\n",
    "    for i in range(B):\n",
    "        render_data_block = {\n",
    "            \"audio\": audio,  # 2 x T\n",
    "            \"body_motion\": pose_results[i, ...],  # T x 104\n",
    "            \"face_motion\": face_results[i, ...],  # T x 256\n",
    "        }\n",
    "        a2p_model.body_renderer.render_full_video(\n",
    "            render_data_block, \n",
    "            os.path.join(a2p_model.output_dir, f\"sample{i}\"), \n",
    "            audio_sr=48_000\n",
    "        )\n",
    "    #     results += [gr.Video(value=f\"/tmp/sample{i}_pred.mp4\", visible=True)]\n",
    "    # results += [gr.Video(visible=False) for _ in range(B, 10)]\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_args checkpoints/diffusion/c1_face/args.json\n",
      "pose_args:  checkpoints/diffusion/c1_pose/args.json\n",
      "face_ckpt checkpoints/diffusion/c1_face/model000155000.pt\n",
      "pose_ckpt checkpoints/diffusion/c1_pose/model000340000.pt\n",
      "running on... cuda:0\n",
      "\u001b[92m adding lip conditioning ./assets/iter-0200000.pt\u001b[00m\n",
      "Loading checkpoints from [checkpoints/diffusion/c1_face/model000155000.pt]...\n",
      "running on... cuda:0\n",
      "guide checkpoint checkpoints/guide/c1_pose/checkpoints/iter-0100000.pt\n",
      "\u001b[92m using keyframes: torch.Size([1, 20, 256])\u001b[00m\n",
      "loading checkpoint from checkpoints/vq/c1_pose/net_iter300000.pth\n",
      "\u001b[92m loading TRANSFORMER checkpoint from checkpoints/guide/c1_pose/checkpoints/iter-0100000.pt\u001b[00m\n",
      "Loading checkpoints from [checkpoints/diffusion/c1_pose/model000340000.pt]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/nexus-scratch/vla/micromamba/envs/a2p_env/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "WARNING:visualize.ca_body.nn.color_cal:Requested color-calibration identity camera not present, defaulting to 400883.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading... checkpoints/ca_body/data/PXB184/body_dec.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.A2PModel at 0x7fc846e28790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2PModel('PXB184')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/diffusion/c2_face/args.json\n",
      "checkpoints/diffusion/c2_pose/args.json\n",
      "checkpoints/diffusion/c2_face/model000190000.pt\n",
      "checkpoints/diffusion/c2_pose/model000190000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.A2PModel at 0x7f9b18646970>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2PModel('RLW104')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/diffusion/c3_face/args.json\n",
      "checkpoints/diffusion/c3_pose/args.json\n",
      "checkpoints/diffusion/c3_face/model000180000.pt\n",
      "checkpoints/diffusion/c3_pose/model000455000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.A2PModel at 0x7f9b0b436460>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2PModel('TXB805')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/diffusion/c4_face/args.json\n",
      "checkpoints/diffusion/c4_pose/args.json\n",
      "checkpoints/diffusion/c4_face/model000185000.pt\n",
      "checkpoints/diffusion/c4_pose/model000350000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.A2PModel at 0x7f9b0b436a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2PModel('GQS883')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2p",
   "language": "python",
   "name": "a2p"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
